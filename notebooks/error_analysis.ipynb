{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis: Incident Triage LLM\n",
    "\n",
    "This notebook analyzes failure cases and identifies patterns in model errors to guide future improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sys\n",
    "\n",
    "# Add scripts to path\n",
    "sys.path.append('../scripts')\n",
    "from prompt_template import format_incident_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "model_path = \"../results/config_c_(higher_capacity)/final-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = []\n",
    "with open('../data/final/test.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions and Identify Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_output(text: str) -> str:\n",
    "    \"\"\"Normalize model output.\"\"\"\n",
    "    text = text.replace(\""\", '\"').replace(\""\", '\"')\n",
    "    text = text.replace(\"'\", \"'\").replace(\"'\", \"'\")\n",
    "    text = text.replace(\"`\", '\"')\n",
    "    return text.strip()\n",
    "\n",
    "def extract_severity(text: str) -> str:\n",
    "    \"\"\"Extract severity.\"\"\"\n",
    "    match = re.search(r'\\bSEV-(1|3)\\b', text, re.IGNORECASE)\n",
    "    return f\"SEV-{match.group(1)}\" if match else \"\"\n",
    "\n",
    "def extract_likely_cause(text: str) -> str:\n",
    "    \"\"\"Extract likely cause.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    if \"block serving\" in text_lower:\n",
    "        return \"Block serving exception\"\n",
    "    elif \"packet responder\" in text_lower:\n",
    "        return \"Packet responder termination\"\n",
    "    return \"\"\n",
    "\n",
    "def parse_output(raw_output: str) -> dict:\n",
    "    \"\"\"Parse model output.\"\"\"\n",
    "    clean_text = normalize_output(raw_output)\n",
    "    \n",
    "    first_brace = clean_text.find('{')\n",
    "    last_brace = clean_text.rfind('}')\n",
    "    \n",
    "    if first_brace != -1 and last_brace != -1:\n",
    "        json_candidate = clean_text[first_brace:last_brace + 1]\n",
    "    else:\n",
    "        json_candidate = clean_text\n",
    "    \n",
    "    try:\n",
    "        parsed = json.loads(json_candidate)\n",
    "        if all(k in parsed for k in [\"severity\", \"likely_cause\", \"recommended_action\"]):\n",
    "            return parsed\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        \"severity\": extract_severity(clean_text),\n",
    "        \"likely_cause\": extract_likely_cause(clean_text),\n",
    "        \"recommended_action\": \"\"\n",
    "    }\n",
    "\n",
    "def generate_prediction(incident_text: str):\n",
    "    \"\"\"Generate prediction.\"\"\"\n",
    "    prompt = format_incident_prompt(incident_text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_new_tokens=256,\n",
    "        min_new_tokens=60,\n",
    "        num_beams=4,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        length_penalty=0.8,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all test samples\n",
    "results = []\n",
    "\n",
    "for example in test_data:\n",
    "    # Extract incident text\n",
    "    prompt_text = example[\"prompt\"]\n",
    "    if \"Incident:\\n\" in prompt_text:\n",
    "        incident_text = prompt_text.split(\"Incident:\\n\", 1)[1]\n",
    "        if \"\\n\\nSTRICT REQUIREMENTS:\" in incident_text:\n",
    "            incident_text = incident_text.split(\"\\n\\nSTRICT REQUIREMENTS:\")[0]\n",
    "    else:\n",
    "        incident_text = prompt_text\n",
    "    \n",
    "    gt = json.loads(example[\"response\"])\n",
    "    \n",
    "    # Generate prediction\n",
    "    raw_output = generate_prediction(incident_text)\n",
    "    prediction = parse_output(raw_output)\n",
    "    \n",
    "    # Check correctness\n",
    "    severity_match = prediction.get(\"severity\") == gt.get(\"severity\")\n",
    "    cause_match = prediction.get(\"likely_cause\") == gt.get(\"likely_cause\")\n",
    "    action_match = prediction.get(\"recommended_action\") == gt.get(\"recommended_action\")\n",
    "    \n",
    "    results.append({\n",
    "        \"incident_text\": incident_text,\n",
    "        \"ground_truth\": gt,\n",
    "        \"prediction\": prediction,\n",
    "        \"raw_output\": raw_output,\n",
    "        \"severity_correct\": severity_match,\n",
    "        \"cause_correct\": cause_match,\n",
    "        \"action_correct\": action_match,\n",
    "        \"exact_match\": severity_match and cause_match and action_match\n",
    "    })\n",
    "\n",
    "print(f\"Generated predictions for {len(results)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify errors\n",
    "errors = [r for r in results if not r[\"exact_match\"]]\n",
    "\n",
    "print(f\"Total errors: {len(errors)} out of {len(results)} ({len(errors)/len(results)*100:.1f}%)\")\n",
    "print(f\"\\nError breakdown:\")\n",
    "print(f\"  Severity errors: {sum(1 for r in results if not r['severity_correct'])}\")\n",
    "print(f\"  Likely cause errors: {sum(1 for r in results if not r['cause_correct'])}\")\n",
    "print(f\"  Recommended action errors: {sum(1 for r in results if not r['action_correct'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Severity Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze severity errors\n",
    "severity_errors = [r for r in results if not r[\"severity_correct\"]]\n",
    "\n",
    "print(f\"Severity misclassifications: {len(severity_errors)}\")\n",
    "\n",
    "if severity_errors:\n",
    "    print(\"\\nPatterns in severity errors:\")\n",
    "    for i, err in enumerate(severity_errors, 1):\n",
    "        gt_sev = err[\"ground_truth\"][\"severity\"]\n",
    "        pred_sev = err[\"prediction\"].get(\"severity\", \"MISSING\")\n",
    "        print(f\"\\n[{i}] Predicted {pred_sev}, Expected {gt_sev}\")\n",
    "        print(f\"    Incident preview: {err['incident_text'][:100]}...\")\n",
    "        \n",
    "        # Check for keywords\n",
    "        text_lower = err['incident_text'].lower()\n",
    "        if \"exception\" in text_lower:\n",
    "            print(f\"    Contains 'exception' keyword\")\n",
    "        warn_count = err['incident_text'].lower().count('warn')\n",
    "        print(f\"    WARN count: {warn_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Likely Cause Confusion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze likely cause errors\n",
    "cause_errors = [r for r in results if not r[\"cause_correct\"]]\n",
    "\n",
    "print(f\"Likely cause errors: {len(cause_errors)}\")\n",
    "\n",
    "if cause_errors:\n",
    "    print(\"\\nPatterns in likely cause errors:\")\n",
    "    \n",
    "    # Build confusion pairs\n",
    "    confusion_pairs = Counter()\n",
    "    for err in cause_errors:\n",
    "        gt = err[\"ground_truth\"][\"likely_cause\"]\n",
    "        pred = err[\"prediction\"].get(\"likely_cause\", \"MISSING\")\n",
    "        confusion_pairs[(gt, pred)] += 1\n",
    "    \n",
    "    print(\"\\nConfusion pairs (ground_truth → prediction):\")\n",
    "    for (gt, pred), count in confusion_pairs.most_common():\n",
    "        print(f\"  {gt} → {pred}: {count} times\")\n",
    "    \n",
    "    print(\"\\nExample errors:\")\n",
    "    for i, err in enumerate(cause_errors[:3], 1):\n",
    "        print(f\"\\n[{i}] Ground truth: {err['ground_truth']['likely_cause']}\")\n",
    "        print(f\"    Prediction: {err['prediction'].get('likely_cause', 'MISSING')}\")\n",
    "        print(f\"    Incident: {err['incident_text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Incomplete JSON Generation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze incomplete outputs\n",
    "incomplete_outputs = []\n",
    "\n",
    "for r in results:\n",
    "    raw = r[\"raw_output\"]\n",
    "    \n",
    "    # Check if output is incomplete\n",
    "    is_incomplete = False\n",
    "    \n",
    "    # Missing braces\n",
    "    if raw.count('{') != raw.count('}'):\n",
    "        is_incomplete = True\n",
    "    \n",
    "    # Missing recommended_action value\n",
    "    if 'recommended_action' in raw and not r['prediction'].get('recommended_action'):\n",
    "        is_incomplete = True\n",
    "    \n",
    "    # Contains UNKNOWN\n",
    "    if any('UNKNOWN' in str(r['prediction'].get(k, '')) for k in ['severity', 'likely_cause', 'recommended_action']):\n",
    "        is_incomplete = True\n",
    "    \n",
    "    if is_incomplete:\n",
    "        incomplete_outputs.append(r)\n",
    "\n",
    "print(f\"Incomplete outputs: {len(incomplete_outputs)} out of {len(results)}\")\n",
    "\n",
    "if incomplete_outputs:\n",
    "    print(\"\\nExample incomplete outputs:\")\n",
    "    for i, inc in enumerate(incomplete_outputs[:3], 1):\n",
    "        print(f\"\\n[{i}] Raw output:\")\n",
    "        print(f\"    {inc['raw_output'][:200]}...\")\n",
    "        print(f\"    Issue: Missing/incomplete recommended_action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlate Errors with Incident Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze if errors correlate with incident length\n",
    "error_lengths = [len(r['incident_text'].split('\\n')) for r in results if not r['exact_match']]\n",
    "correct_lengths = [len(r['incident_text'].split('\\n')) for r in results if r['exact_match']]\n",
    "\n",
    "if error_lengths and correct_lengths:\n",
    "    avg_error_length = sum(error_lengths) / len(error_lengths)\n",
    "    avg_correct_length = sum(correct_lengths) / len(correct_lengths)\n",
    "    \n",
    "    print(\"Incident length analysis:\")\n",
    "    print(f\"  Average lines in errors: {avg_error_length:.1f}\")\n",
    "    print(f\"  Average lines in correct: {avg_correct_length:.1f}\")\n",
    "    print(f\"  Difference: {avg_error_length - avg_correct_length:+.1f} lines\")\n",
    "\n",
    "# Check keyword correlation\n",
    "print(\"\\nKeyword presence in errors vs correct predictions:\")\n",
    "keywords = ['exception', 'warn', 'error', 'terminating', 'serving']\n",
    "\n",
    "for keyword in keywords:\n",
    "    error_has_kw = sum(1 for r in results if not r['exact_match'] and keyword in r['incident_text'].lower())\n",
    "    correct_has_kw = sum(1 for r in results if r['exact_match'] and keyword in r['incident_text'].lower())\n",
    "    \n",
    "    error_rate = (error_has_kw / len([r for r in results if not r['exact_match']])) * 100 if errors else 0\n",
    "    correct_rate = (correct_has_kw / len([r for r in results if r['exact_match']])) * 100 if any(r['exact_match'] for r in results) else 0\n",
    "    \n",
    "    print(f\"  '{keyword}': Errors={error_rate:.1f}%, Correct={correct_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary of Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ERROR PATTERN SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. SEVERITY MISCLASSIFICATION\")\n",
    "print(\"   Pattern: Model occasionally predicts SEV-3 for incidents containing 'exception while serving'\")\n",
    "print(\"   Root cause: Small dataset (33 training samples) limits pattern learning\")\n",
    "print(\"   Mitigation: Heuristic override for 'got exception while serving' → SEV-1\")\n",
    "\n",
    "print(\"\\n2. INCOMPLETE JSON GENERATION\")\n",
    "print(\"   Pattern: Model stops after 'recommended_action:' without completing the value\")\n",
    "print(\"   Root cause: Small dataset + limited token budget in early training\")\n",
    "print(\"   Mitigation: Increased max_new_tokens=256, added min_new_tokens=60, length_penalty=0.8\")\n",
    "\n",
    "print(\"\\n3. FORMATTING ISSUES\")\n",
    "print(\"   Pattern: Smart quotes, parentheses around values, missing quotes\")\n",
    "print(\"   Root cause: Model trained on standard JSON but generates with formatting variations\")\n",
    "print(\"   Mitigation: Robust normalization + value cleaning in inference\")\n",
    "\n",
    "print(\"\\n4. LIKELY CAUSE CONFUSION\")\n",
    "print(\"   Pattern: Confuses 'Packet responder termination' and 'Block serving exception'\")\n",
    "print(\"   Root cause: Both appear in similar HDFS DataNode contexts\")\n",
    "print(\"   Mitigation: Keyword-based extraction with context hints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Suggested Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUGGESTED IMPROVEMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. INCREASE TRAINING DATA (High Priority)\")\n",
    "print(\"   - Use sliding window preprocessing (stride=2-4) to generate 100-200 incidents\")\n",
    "print(\"   - Current: 48 incidents → Target: 150+ incidents\")\n",
    "print(\"   - Expected impact: +15-20% accuracy, reduced repair rate\")\n",
    "\n",
    "print(\"\\n2. UPGRADE TO LARGER MODEL (Medium Priority)\")\n",
    "print(\"   - Current: FLAN-T5-small (80M params)\")\n",
    "print(\"   - Target: FLAN-T5-base (250M params)\")\n",
    "print(\"   - Expected impact: Better JSON formatting, fewer incomplete outputs\")\n",
    "\n",
    "print(\"\\n3. ADD DATA AUGMENTATION (Medium Priority)\")\n",
    "print(\"   - Paraphrase incident descriptions using GPT\")\n",
    "print(\"   - Vary log order while preserving labels\")\n",
    "print(\"   - Expected impact: +10% generalization\")\n",
    "\n",
    "print(\"\\n4. IMPLEMENT CONSTRAINED DECODING (Low Priority)\")\n",
    "print(\"   - Force JSON format using grammar-based generation\")\n",
    "print(\"   - Libraries: guidance-ai, jsonformer\")\n",
    "print(\"   - Expected impact: 100% valid JSON, no repair needed\")\n",
    "\n",
    "print(\"\\n5. MULTI-TASK LEARNING (Low Priority)\")\n",
    "print(\"   - Train on auxiliary tasks: log level prediction, component classification\")\n",
    "print(\"   - Expected impact: Better representation learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save error analysis results\n",
    "error_analysis = {\n",
    "    \"total_samples\": len(results),\n",
    "    \"total_errors\": len(errors),\n",
    "    \"error_rate\": (len(errors) / len(results)) * 100,\n",
    "    \"error_breakdown\": {\n",
    "        \"severity_errors\": sum(1 for r in results if not r['severity_correct']),\n",
    "        \"cause_errors\": sum(1 for r in results if not r['cause_correct']),\n",
    "        \"action_errors\": sum(1 for r in results if not r['action_correct'])\n",
    "    },\n",
    "    \"patterns\": [\n",
    "        \"Severity misclassification for exception-containing incidents\",\n",
    "        \"Incomplete JSON generation (stops mid-field)\",\n",
    "        \"Smart quotes and parentheses formatting issues\",\n",
    "        \"Confusion between Packet responder and Block serving causes\"\n",
    "    ],\n",
    "    \"improvements\": [\n",
    "        \"Increase training data to 150+ samples via sliding window\",\n",
    "        \"Upgrade to FLAN-T5-base for better capacity\",\n",
    "        \"Add data augmentation with paraphrasing\",\n",
    "        \"Implement constrained decoding for guaranteed JSON\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_file = \"../results/error_analysis.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(error_analysis, f, indent=2)\n",
    "\n",
    "print(f\"✓ Error analysis saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
